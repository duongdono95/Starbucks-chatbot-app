{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owner\\OneDrive\\Desktop\\Study\\MDS\\Year 2\\ProjectBasedLearning\\Project1\\chatbot\\venv\\Lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "with open (\"dataset.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "features, targets = [], []\n",
    "classes = set()\n",
    "responses = dict()\n",
    "\n",
    "# print(nlp.pipe_names)       \n",
    "def clean_text(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    cleaned_doc = [token.lemma_ for token in doc if not token.is_punct and not token.is_space and token.is_alpha]\n",
    "    return \" \".join(cleaned_doc)\n",
    "\n",
    "for item in data:\n",
    "    classes.add(item[\"intent\"])\n",
    "    for t in item[\"text\"]:\n",
    "        features.append(t)\n",
    "        targets.append(item[\"intent\"])\n",
    "    responses[item[\"intent\"]] = []\n",
    "    for res in item[\"responses\"]:\n",
    "        responses[item[\"intent\"]].append(res)\n",
    "\n",
    "le = LabelEncoder()\n",
    "labeled_target = le.fit_transform(targets)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'attribute_ruler', 'ner'])\n",
    "vectorise_model = TfidfVectorizer( lowercase=True, ngram_range=(1,2))\n",
    "\n",
    "df = pd.DataFrame({\"target\": targets, \"feature\": features})\n",
    "df[\"labeled_target\"] = labeled_target\n",
    "df[\"cleaned_feature\"] = df[\"feature\"].apply(lambda text: clean_text(text, nlp))\n",
    "vectorised_text = vectorise_model.fit_transform(df[\"cleaned_feature\"])\n",
    "vectorised_text = vectorised_text.toarray()\n",
    "num_vectorised_features = vectorised_text.shape[1]\n",
    "vectorised_columns = [f\"vectorised_feature_{i+1}\" for i in range(num_vectorised_features)]\n",
    "vectorised_df = pd.DataFrame(vectorised_text, columns=vectorised_columns)\n",
    "df = pd.concat([df, vectorised_df], axis=1)\n",
    "df.to_csv(\"cleaned_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owner\\OneDrive\\Desktop\\Study\\MDS\\Year 2\\ProjectBasedLearning\\Project1\\chatbot\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best parameter value(s): {'alpha': 0.01}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Ensure that df[\"vectorised_feature\"] is a list of lists (convert it to a proper NumPy array)\n",
    "x = df.drop(columns=[\"target\", \"feature\", \"cleaned_feature\"])# Stack the vectors into a 2D NumPy array\n",
    "y = df[\"labeled_target\"].values  # Convert to NumPy array\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=42)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "params = {'alpha': [0.01, 0.5, 0.07, 0.1, 0.5, 1.0, 10.0]}\n",
    "\n",
    "# Initialize and fit GridSearchCV\n",
    "nb_grid = GridSearchCV(MultinomialNB(), params, n_jobs=-1, cv=5, verbose=5)\n",
    "nb_grid.fit(x_train, y_train)\n",
    "# Print best parameters\n",
    "print('Best parameter value(s): {}'.format(nb_grid.best_params_))\n",
    "model = nb_grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Accuracy Score: 1.0\n",
      "Training - Accuracy Score: 0.8620689655172413\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "train_preds = model.predict(x_train)\n",
    "print('Training - Accuracy Score: {}'.format(metrics.accuracy_score(y_train, train_preds)))\n",
    "\n",
    "val_preds = model.predict(x_test)\n",
    "print('Training - Accuracy Score: {}'.format(metrics.accuracy_score(y_test, val_preds)))\n",
    "\n",
    "# Training - Accuracy Score: 1.0\n",
    "# Training - Accuracy Score: 0.8620689655172413"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
